activation: SiLU
beta: 0.2
hidden_dims:
- 256
- 256
- 256
- 256
input_dim: 16
latent_dim: 4
loss_weights: null
lr: 0.0001
run_name: vanilla_vae_latent_dim=4_beta=0.2_lr=0.0001
