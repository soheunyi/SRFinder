{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "current_pwd = os.getcwd()\n",
    "\n",
    "possible_paths = [\n",
    "    '/home/export/soheuny/SRFinder/soheun/notebooks', \n",
    "    '/home/soheuny/HH4bsim/soheun/notebooks'\n",
    "]\n",
    "    \n",
    "assert os.getcwd() in possible_paths, f\"Did you change the path? It should be one of {possible_paths}\"\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import torch\n",
    "\n",
    "from plots import hist_events_by_labels\n",
    "from events_data import EventsData\n",
    "from fvt_classifier import FvTClassifier\n",
    "from tst_info import TSTInfo\n",
    "# import LogNorm\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "\n",
    "features = [\n",
    "    \"sym_Jet0_pt\", \"sym_Jet1_pt\", \"sym_Jet2_pt\", \"sym_Jet3_pt\",\n",
    "    \"sym_Jet0_eta\", \"sym_Jet1_eta\", \"sym_Jet2_eta\", \"sym_Jet3_eta\",\n",
    "    \"sym_Jet0_phi\", \"sym_Jet1_phi\", \"sym_Jet2_phi\", \"sym_Jet3_phi\",  \n",
    "    \"sym_Jet0_m\", \"sym_Jet1_m\", \"sym_Jet2_m\", \"sym_Jet3_m\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/export/soheuny/.conda/envs/coffea_torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from events_data import events_from_scdinfo\n",
    "from tst_info import TSTInfo\n",
    "from debiasing import get_bias_fn, get_histograms\n",
    "from plots import hist_events_by_labels\n",
    "import tqdm\n",
    "from matplotlib.colors import LogNorm\n",
    "from dataset import generate_mother_dataset, split_scdinfo\n",
    "import pytorch_lightning as pl\n",
    "from signal_region import get_SR_stats\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from training_info import TrainingInfoV2\n",
    "from plots import calibration_plot, plot_rewighted_samples_by_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from ancillary_features import get_m4j\n",
    "from pl_callbacks import CalibrationPlotCallback, ReweightedPlotCallback\n",
    "\n",
    "# use tex\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = \"Times New Roman\"\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.titlesize\"] = 20\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"figure.labelsize\"] = 20\n",
    "plt.rcParams[\"lines.markersize\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_3b=1000000, signal_ratio=0.0, seed=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 0\n",
      "[rank: 0] Seed set to 0\n",
      "/home/export/soheuny/.conda/envs/coffea_torch/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/export/soheuny/.conda/envs/coffea_torch/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/training_ablation_2/bs=True_bs_milestones=(1, 3, 6, 10, 15)_init_lr=0.01_lrs=True_min_lr=0.001_lr_factor=0.5_lr_patience=15_seed=0_timestamp=1728770507\n",
      "/home/export/soheuny/.conda/envs/coffea_torch/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/export/soheuny/SRFinder/soheun/data/tmp/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type                | Params\n",
      "-------------------------------------------------------------\n",
      "0 | encoder              | FvTEncoder          | 2.0 K \n",
      "1 | attention_classifier | AttentionClassifier | 24    \n",
      "-------------------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "121       Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary checkpoint callback\n",
      "Epoch 0: 100%|██████████| 1028/1028 [00:48<00:00, 21.38it/s, v_num=0, 1000x_val_loss_first_digits=664.0, 1000x_val_loss_second_digits=0.0832, val_sigma_sq=284.0, lr=0.010]Batch size updated to: 2048\n",
      "Epoch 2: 100%|██████████| 514/514 [00:30<00:00, 16.66it/s, v_num=0, 1000x_val_loss_first_digits=660.0, 1000x_val_loss_second_digits=0.995, val_sigma_sq=89.60, lr=0.010, train_loss_lower_digits=661.0, train_loss_second_digits=0.512] Batch size updated to: 4096\n",
      "Epoch 5: 100%|██████████| 257/257 [00:21<00:00, 11.99it/s, v_num=0, 1000x_val_loss_first_digits=661.0, 1000x_val_loss_second_digits=0.475, val_sigma_sq=177.0, lr=0.010, train_loss_lower_digits=660.0, train_loss_second_digits=0.501]Batch size updated to: 8192\n",
      "Epoch 9: 100%|██████████| 129/129 [00:18<00:00,  6.86it/s, v_num=0, 1000x_val_loss_first_digits=660.0, 1000x_val_loss_second_digits=0.0646, val_sigma_sq=45.00, lr=0.010, train_loss_lower_digits=659.0, train_loss_second_digits=0.873]Batch size updated to: 16384\n",
      "Epoch 14: 100%|██████████| 65/65 [00:18<00:00,  3.44it/s, v_num=0, 1000x_val_loss_first_digits=659.0, 1000x_val_loss_second_digits=0.538, val_sigma_sq=14.20, lr=0.010, train_loss_lower_digits=659.0, train_loss_second_digits=0.493]  Batch size updated to: 32768\n",
      "Epoch 15:   0%|          | 0/33 [00:00<?, ?it/s, v_num=0, 1000x_val_loss_first_digits=659.0, 1000x_val_loss_second_digits=0.538, val_sigma_sq=14.20, lr=0.010, train_loss_lower_digits=659.0, train_loss_second_digits=0.493]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/export/soheuny/.conda/envs/coffea_torch/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (33) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 33/33 [00:20<00:00,  1.64it/s, v_num=0, 1000x_val_loss_first_digits=659.0, 1000x_val_loss_second_digits=0.507, val_sigma_sq=4.040, lr=0.001, train_loss_lower_digits=658.0, train_loss_second_digits=0.571] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 33/33 [00:20<00:00,  1.64it/s, v_num=0, 1000x_val_loss_first_digits=659.0, 1000x_val_loss_second_digits=0.507, val_sigma_sq=4.040, lr=0.001, train_loss_lower_digits=658.0, train_loss_second_digits=0.571]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "n_3b = 100_0000\n",
    "device = torch.device(\"cuda\")\n",
    "experiment_name = \"counting_test_v2\"\n",
    "signal_filename = \"HH4b_picoAOD.h5\"\n",
    "ratio_4b = 0.5\n",
    "\n",
    "seeds = [0]\n",
    "hparam_filter = {\n",
    "    \"experiment_name\": lambda x: x in [experiment_name],\n",
    "    \"n_3b\": n_3b,\n",
    "    \"seed\": lambda x: x in seeds,\n",
    "    \"signal_ratio\": 0.0,\n",
    "}\n",
    "config = {\n",
    "            \"batch_schedule\": True,\n",
    "            \"batch_milestones\": (1, 3, 6, 10, 15),\n",
    "            \"init_lr\": 1e-2,\n",
    "            \"lr_schedule\": True,\n",
    "            \"min_lr\": 1e-3,\n",
    "            \"lr_factor\": 0.5,\n",
    "            \"lr_patience\": 15,\n",
    "            \"depth\": {\"encoder\": 4, \"decoder\": 1},\n",
    "        }\n",
    "hashes = TSTInfo.find(hparam_filter, sort_by=[\"seed\", \"signal_ratio\"])\n",
    "\n",
    "for tstinfo_hash in hashes:\n",
    "    tstinfo = TSTInfo.load(tstinfo_hash)\n",
    "    seed = tstinfo.hparams[\"seed\"]\n",
    "    print(\n",
    "        f\"n_3b={tstinfo.hparams['n_3b']}, signal_ratio={tstinfo.hparams['signal_ratio']}, seed={tstinfo.hparams['seed']}\"\n",
    "    )\n",
    "    base_fvt_tinfo_hash = tstinfo.base_fvt_tinfo_hash\n",
    "    base_fvt_tinfo = TrainingInfoV2.load(base_fvt_tinfo_hash)\n",
    "\n",
    "    train_scdinfo, val_scdinfo = base_fvt_tinfo.fetch_train_val_scdinfo()\n",
    "    events_train = events_from_scdinfo(train_scdinfo, features, signal_filename)\n",
    "    events_val = events_from_scdinfo(val_scdinfo, features, signal_filename)\n",
    "    events_tst = events_from_scdinfo(tstinfo.scdinfo_tst, features, signal_filename)\n",
    "    events_train.shuffle(seed=seed)\n",
    "    events_val.shuffle(seed=seed)\n",
    "    events_tst.shuffle(seed=seed)\n",
    "\n",
    "    # batch_size = base_fvt_tinfo.hparams[\"batch_size\"] # double the batch size to fit the kernel matrix\n",
    "    batch_size = 2**10\n",
    "\n",
    "    events_train.fit_batch_size(batch_size)\n",
    "    events_val.fit_batch_size(batch_size)\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    batch_schedule = config[\"batch_schedule\"]\n",
    "    batch_milestones = config[\"batch_milestones\"]\n",
    "    init_lr = config[\"init_lr\"]\n",
    "    lr_schedule = config[\"lr_schedule\"]\n",
    "    min_lr = config[\"min_lr\"]\n",
    "    lr_factor = config[\"lr_factor\"]\n",
    "    lr_patience = config[\"lr_patience\"]\n",
    "    run_name = f\"bs={batch_schedule}_bs_milestones={batch_milestones}_init_lr={init_lr}_lrs={lr_schedule}_min_lr={min_lr}_lr_factor={lr_factor}_lr_patience={lr_patience}\"\n",
    "\n",
    "    # IMPORTANT: For reproducibility, weight initialization is fixed\n",
    "    pl.seed_everything(seed)\n",
    "    base_model_new = FvTClassifier(\n",
    "        num_classes=2,\n",
    "        dim_input_jet_features=4,\n",
    "        dim_dijet_features=base_fvt_tinfo.hparams[\"dim_dijet_features\"],\n",
    "        dim_quadjet_features=base_fvt_tinfo.hparams[\"dim_quadjet_features\"],\n",
    "        run_name=f\"{run_name}_seed={seed}_timestamp={timestamp}\",\n",
    "        device=device,\n",
    "        depth=config[\"depth\"],\n",
    "    )\n",
    "\n",
    "    if lr_schedule:\n",
    "        lr_scheduler_config = {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"factor\": lr_factor,\n",
    "            \"threshold\": 0.0001,\n",
    "            \"patience\": lr_patience,\n",
    "            \"cooldown\": 1,\n",
    "            \"min_lr\": min_lr,\n",
    "        }\n",
    "    else:\n",
    "        lr_scheduler_config = {\"type\": \"none\"}\n",
    "\n",
    "    if batch_schedule:\n",
    "        dataloader_config = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"batch_size_milestones\": batch_milestones,\n",
    "            \"batch_size_multiplier\": 2,\n",
    "        }\n",
    "    else:\n",
    "        dataloader_config = {\"batch_size\": batch_size}\n",
    "\n",
    "    base_model_new.fit(\n",
    "        events_train.to_tensor_dataset(),\n",
    "        events_val.to_tensor_dataset(),\n",
    "        max_epochs=200,\n",
    "        train_seed=seed,\n",
    "        save_checkpoint=False,\n",
    "        optimizer_config={\"type\": \"Adam\", \"lr\": init_lr},\n",
    "        lr_scheduler_config=lr_scheduler_config,\n",
    "        dataloader_config=dataloader_config,\n",
    "        tb_log_dir=\"training_ablation_2\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
